{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gordon Doore and Ghailan Fadah**\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 1: Single-layer networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from adaline import Adaline\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "def plot_adaline_train(net, loss_list, acc_list, plotMarkers=False, title='ADALINE'):\n",
    "    n_epochs = len(loss_list)\n",
    "    \n",
    "    x = np.arange(1, n_epochs+1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'{title} ({n_epochs} epochs)')\n",
    "    \n",
    "    curveStr = '-r'\n",
    "    if plotMarkers:\n",
    "        curveStr += 'o'\n",
    "    \n",
    "    ax1.plot(x, loss_list, curveStr)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (Sum squared error)')\n",
    "    ax2.plot(x, acc_list, curveStr)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paste in your code to load Old Faithful data with standardized features below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faithful = pd.read_csv('data/old_faithful.csv')\n",
    "X_pre = faithful[['eruptions','waiting']]\n",
    "y = faithful[['severe']]\n",
    "X_stand = (X_pre - X_pre.mean(axis = 0))/(X_pre.std(axis = 0))\n",
    "X_stand = X_stand.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Neural network for regression\n",
    "\n",
    "Given ADALINE's linear (identity) activation and sum-of-squares loss function, the learned weights can be used for more than just classification. In this task, you will use ADALINE to perform a linear regression (*the same neural network offers another interpretation of what you did in CS251!*).\n",
    "\n",
    "### Goal\n",
    "\n",
    "Your goal is to get an ADALINE network to predict `waiting` from `eruptions`. That is, you are setting up a simple (bivariate) linear regression with equation $$y_i = m \\times x_i + b$$where the \"x\" variable (*predictor variable*) is `eruptions` and the \"y\" variable (*response variable*) is `waiting` (raw).\n",
    "\n",
    "**Ultimately, you want to draw a regression line *through* the Old Faithful data clusters to *join* rather than divide them.**\n",
    "\n",
    "### Design\n",
    "\n",
    "You can do the regression with the exact network you have currently implemented. **You shouldn't make any code changes to your `Adaline` class.**  In the cell below, use your network to set up the regression by making appropriate design choices:\n",
    "- Network input features: How many? What should they be?\n",
    "- Weights: What do they mean in this problem context?\n",
    "- What are the \"classes\"?\n",
    "\n",
    "### Tips\n",
    "\n",
    "- I suggest using the standardized version of the predictor (otherwise you may run into numeric stability issues), but it's fine to use the raw/unstandardized response variable. \n",
    "- Default hyperparameters should work well.\n",
    "- You may need to add a singleton dimension after selecting your input predictor feature below so that your existing code works i.e. `shape=(272,1)`, NOT `shape=(272,)`\n",
    "\n",
    "**Write your training code in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_x = X_stand[:, 0].reshape(272,1)\n",
    "response_y = X_pre[\"waiting\"].to_numpy()\n",
    "\n",
    "ada = Adaline()\n",
    "loss_list, acc_list = ada.fit(predictor_x, response_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, **create a scatter plot of the data and the overlayed regression line**. Have the x-axis map onto standardized `eruptions` and y-axis onto raw `waiting` values.\n",
    "\n",
    "### Tips\n",
    "- You will need to leverage the model linear equation to go from x values to predicted y values. $y_i = m \\times x_i + b$\n",
    "- Look at the class boundary plot code that you used for classification. You will need to generate linearly spaced x values before plotting your regression y values on your regression line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: ADALINE and logistic regression\n",
    "\n",
    "In this task, you will extend ADALINE to logistic regression, where we explicitly represent the probability of class membership.\n",
    "\n",
    "For example data point $i$ is 80% likely to be in class A and 20% in class B.\n",
    "\n",
    "**Remember:** Despite the name, logistic regression is actually about solving a **classification** problem. So this is more similar to Task 3 than Task 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Implement logistic regression\n",
    "\n",
    "Create a subclass of `Adaline` called `AdalineLogistic` in a new file called `adaline_logistic.py`. **Only override existing methods as needed to make the following changes. DO NOT MODIFY `adaline.py` FOR ANY REASON!!**.\n",
    "\n",
    "#### Design\n",
    "\n",
    "1. Use the sigmoid activation function. $z = f(x) = \\frac{1}{1+e^{-x}}$\n",
    "2. Represent the output classes as 0 or +1. This should require a code change (activation values >=0.5 are classified as 1, otherwise class 0) and preprocessing of the old faithful data.\n",
    "3. Use the cross-entropy loss function: $\\sum_{i=1}^n \\left [ -y_i Log(z_i) - (1-y_i)Log(1 - z_i) \\right ] $\n",
    "where $z_i$ is the activation to input sample $i$ and $y_i$ is the corresponding $i^{th}$ class label (0 or 1).\n",
    "\n",
    "\n",
    "#### Todo below:\n",
    "\n",
    "1. Train your network using the standardized Old Faithful data. Default hyperparameters should work fine.\n",
    "2. Plot your loss and accuracy as a function of epoch.\n",
    "3. Plot the logistic regression decision boundary and the data (Use your code from Task 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 10.** Why do we need to relabel the classes from -1/+1 to 0/1 when training a logistic regression network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 10:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaline_logistic import AdalineLogistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Test point probabilities\n",
    "\n",
    "#### Questions\n",
    "\n",
    "**Question 11.** (a) Determine the probability that the following test points belong to **both classes**:\n",
    "\n",
    "Format: standardized (eruptions, waiting)\n",
    "- (0.4, 0.98)\n",
    "- (0.5, -2)\n",
    "- (-1, 0.5)\n",
    "\n",
    "(b) Interpret what your probabilities make sense in light of decision boundary plot you made in Task 5a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 11:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "**NOTE:** Never integrate extensions into your base project so that it changes the expected behavior of core functions. If your extension changes the core design/behavior, no problem, duplicate your working base project and add features from there.\n",
    "\n",
    "Generally, a small number of \"in-depth\" extensions count for more than many \"shallow\" extensions.\n",
    "\n",
    "1. Extend the ADALINE model to multi-class classification using the One-Vs-Rest (OvR) method. Recall that with this scheme, we train multiple networks with each of the $C$ output classes serving as the +1 class (others set to -1 class). For example, for classes [a, b, c] would would train the networks with the following class labels: [1, -1, -1], [-1, 1, =1], [-1, -1, 1], respectively. We then classify based on the class that generates the highest max probability / activation value. Test it on a dataset with more than two classes (e.g. Iris). *You can adapt this to the logistic regression network, but note the necessary change in the coding of classes.*\n",
    "2. Create plots of the ADALINE regression curve superimposed on the 2D data scatterplot after training on different numbers of epochs. One options is to plot all the curves in a single plot and establish a color scheme for time so that the viewer can visually discern the time sequence. Another possibility is to create a NxM grid of plots showing the progression (be sure to label the titles with #epochs).\n",
    "3. Demonstrate how ADALINE can handle multiple linear regression.\n",
    "4. Test the performance of single layer neural networks at classifying a binary class dataset of your choice.\n",
    "5. Compare the performance of ADALINE, Perceptron, and Logistic Regression single-layer networks in additional ways and/or with additional datasets.\n",
    "6. Research, implement, and analyze a neural network technique called early stopping. In essence, you stop training when the change in loss between successive epoches drops below some threshold. You can make your early stopping implementation fancier. For example, only stop if the change relative to the average loss over the most recent few epochs is less than the tolerance. Why could this be an improvement over the other method?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
