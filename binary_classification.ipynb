{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ghailan and Gordon**\n",
    "\n",
    "Fall 2023\n",
    "\n",
    "CS 343: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from adaline import Adaline\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.show()\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 | Single-Layer Networks\n",
    "\n",
    "In this project, you will implement single-layer neural networks that includes the same fundamental components as larger multi-layer networks. You will get familiar with the most common \"neural network workflow\":\n",
    "- preprocessing data\n",
    "- training a neural network\n",
    "- evaluating test data\n",
    "- examining performance metrics\n",
    "\n",
    "We will take advantage of the relative simplicity of single-layer neural networks to analyze and visualize the learned class decision boundaries (*this is more difficult and less intuitive in more complex neural networks that we will study!*).\n",
    "\n",
    "You will also investigate how the same neural network architecture can be used for both classification and regression with only modest changes.\n",
    "\n",
    "#### Reminders\n",
    "\n",
    "- In this class, use `numpy ndarray` (`np.array()`), not Numpy Matrix.\n",
    "- To help safeguard against data loss when working in a jupyter notebook, make sure the notebook is `Trusted` (Top right corner of notebook when opened in your browser). This will ensure your work autosaves. **I still recommend manually saving at least every few minutes with (Control+S / Cmd+S)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the ADAptive LInear NEuron (ADALINE) network for binary classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Task 1, complete the methods of the `Adaline` class in `adaline.py`. This includes:\n",
    "\n",
    "- `net_input(self, features)`\n",
    "- `activation(self, net_in)`\n",
    "- `compute_loss(self, errors)`\n",
    "- `compute_accuracy(self, y, y_pred)`\n",
    "- `gradient(self, errors, features)`\n",
    "\n",
    "- `predict(self, features)`\n",
    "- `fit(self, features, y, n_epochs, lr)`\n",
    "\n",
    "**Important:** Before starting, read through the method descriptions and expected inputs/outputs. It probabily woud be a good idea to tackle simpler/smaller methods first, then use them in more complex ones. For example, it may be a good idea to work on `net_input` first because it is required to complete `fit`. There is test code below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the ADALINE network equations\n",
    "\n",
    "##### Net input\n",
    "\n",
    "$\\vec{x}_i$ is one of the $N$ data sample vectors from the dataset matrix $A$. That is, `x_i.shape = (M,)`.\n",
    "\n",
    "$$\\text{netIn}_i = \\sum_{j=1}^M x_{ij} w_j + b$$\n",
    "\n",
    "##### Net activation\n",
    "\n",
    "Identity function:\n",
    "\n",
    "$$ f(x) = x $$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\\text{netAct}_i = \\text{netIn}_i$$\n",
    "\n",
    "\n",
    "##### Loss: Sum of squared error\n",
    "\n",
    "$$L(\\vec{w}) = \\frac{1}{2} \\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right )^2 $$\n",
    "\n",
    "##### Gradient (bias)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right )$$\n",
    "\n",
    "##### Gradient (wts)\n",
    "\n",
    "Below, $x_{ij}$ is the $j^{th}$ feature of the data sample vector $\\vec{x}_i$.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_j} = -\\sum_{i=1}^N \\left ( y_i - \\text{netAct}_i \\right ) x_{ij}$$\n",
    "\n",
    "##### Gradient descent (delta rule)\n",
    "\n",
    "$$b(t+1) = b(t) - \\eta \\frac{\\partial L}{\\partial b}$$\n",
    "$$w_j(t+1) = w_j(t) - \\eta \\frac{\\partial L}{\\partial w_j}$$\n",
    "\n",
    "above $\\eta$ is the learning rate, and $N$ is the training set (number of data samples in training epoch)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Test your ADALINE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Adaline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `loss` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your loss is 3.6609344768925496 and it should be 3.6609344768925496\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "randErrors = np.array([-0.835,  0.322, -0.381,  0.496, -0.89 , -0.953])\n",
    "net_act = np.random.rand(len(randErrors))\n",
    "debugLoss = net.compute_loss(randErrors, net_act)\n",
    "print(f'Your loss is {debugLoss} and it should be 3.6609344768925496')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `accuracy` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Your accuracy is 1.0 and it should be 1.0\n",
      "Test 2: Your accuracy is 0.3333333333333333 and it should be 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "randClasses1 = np.where(randErrors >= 0, 1, -1)\n",
    "randClasses2 = np.roll(randClasses1, 1)\n",
    "acc1 = net.compute_accuracy(randClasses1, randClasses1)\n",
    "acc2 = net.compute_accuracy(randClasses1, randClasses2)\n",
    "print(f'Test 1: Your accuracy is {acc1} and it should be 1.0')\n",
    "print(f'Test 2: Your accuracy is {acc2} and it should be 0.33333333333333337')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Your bias gradient is 0.7839944892482784 and it should be 0.7839944892482784\n",
      "Test 2: Your wt gradient is [ 0.4   -0.897  7.689] and it should be [ 0.4   -0.897  7.689]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "randFeatures = np.random.normal(loc=0, scale=1, size=(10,3))\n",
    "randErrors1 = np.random.normal(loc=0, scale=1, size=(10,))\n",
    "randBiasGrad, randWtGrad = net.gradient(randErrors1, randFeatures)\n",
    "print(f'Test 1: Your bias gradient is {randBiasGrad} and it should be 0.7839944892482784')\n",
    "print(f'Test 2: Your wt gradient is {randWtGrad} and it should be [ 0.4   -0.897  7.689]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `predict` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your predicted classes are [-1  1  1  1  1 -1  1  1 -1  1].\n",
      "            They should be [-1  1  1  1  1 -1  1  1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "inputs = np.random.randn(10, 5)\n",
    "net.wts = np.random.randn(5)\n",
    "net.b = np.random.randn(1)\n",
    "y_pred = net.predict(inputs)\n",
    "print(f'Your predicted classes are {y_pred}.\\n            They should be [-1  1  1  1  1 -1  1  1 -1  1]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "net = Adaline()\n",
    "inputs = np.random.randn(10, 5)\n",
    "y = np.sign(np.random.randn(10))\n",
    "loss, acc = net.fit(inputs, y)\n",
    "print(f'Your end-of-training loss / accuracy are\\n{loss[-1]:.4f} / {acc[-1]}.\\nThey should be\\n3.9449 / 0.6')\n",
    "print(f'Your wts after training are:\\n{net.get_wts()}\\nand should be\\n[-0.033  0.33  -0.382 -0.192  0.087]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Load in and preprocess old faithful data\n",
    "\n",
    "In this task, you will be working with the old faithful dataset. Here is a description of the dataset:\n",
    "\n",
    "    Waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA.\n",
    "\n",
    "    Variables:\n",
    "    ----------\n",
    "    sample     numeric      Measurement number\n",
    "    eruptions  numeric      Eruption time in mins\n",
    "    waiting    numeric      Waiting time to next eruption\n",
    "    severe     categorical  Whether the eruption was \"severe\"\n",
    "                            (+1: severe, -1 not severe)\n",
    "\n",
    "Write code to do the following in the below cell.\n",
    "\n",
    "1. Load in `old_faithful.csv`, represent the data using a ndarray. Select the `eruptions` and `waiting` variables for your features.  Shape = [Num samps, Num features] = [272, 2].\n",
    "2. Assign the output classes (**severe**) to a separate 1D ndarray vector. Shape=(272,)\n",
    "3. Preprocess the data by performing min-max normalization across features (i.e. \"per-variable\"/\"separately\").\n",
    "4. Use matplotlib to create a scatter plot of the normalized data, color-coding data points according to their class\n",
    "5. I suggest using pandas, but you're welcome to do this however you like.\n",
    "\n",
    "**Make sure that executing the below cell results in an inline scatter plot, color-coded by class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Train ADALINE on normalized Old Faithful data using default hyperparameters (i.e. learning rate, epochs)\n",
    "\n",
    "Print out the final loss and accuracy, then use the provided function to plot your training results inline in the below cell.\n",
    "\n",
    "By the final epoch, training loss should reach ~11.36 and accuracy ~100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adaline_train(net, loss_list, acc_list, plotMarkers=False, title='ADALINE'):\n",
    "    n_epochs = len(loss_list)\n",
    "    \n",
    "    x = np.arange(1, n_epochs+1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'{title} ({n_epochs} epochs)')\n",
    "    \n",
    "    curveStr = '-r'\n",
    "    if plotMarkers:\n",
    "        curveStr += 'o'\n",
    "    \n",
    "    ax1.plot(x, loss_list, curveStr)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss (Sum squared error)')\n",
    "    ax2.plot(x, acc_list, curveStr)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 1.** Based on your loss and accuracy curves, does it look like your network learned to classify the old faithful data? Why or why not?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature scaling and hyperparameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Feature scaling\n",
    "\n",
    "Copy your code from Task 1 to import the Old Faithful data, but this time don't normalize before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 2.** What happens to the loss when we don't normalize the features before training? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Test how individually standardizing your features affects the rate at which loss decreases over epochs\n",
    "\n",
    "1. Write code in the cell below to train the network on standardized features. Recall that standardizing a variable means applying the transformation $\\frac{x - \\mu}{\\sigma}$. The mean and standard deviation should be computed over the entire dataset and separately per feature.\n",
    "2. Plot the loss and accuracy.\n",
    "\n",
    "**The cell should generate an inline pair of plots when executed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 3.** Explain the similarities/differences in loss and accuracy curves between these plots and those that you made in Task 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Influence of learning rate\n",
    "\n",
    "This subtask focuses on the influence of learning rate (a model **hyperparameter**) on the quality of neural network training.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "**Question 4:** Make small changes to the learning rate hyperparameter below. How does it affect the loss?\n",
    "\n",
    "**Question 5:** What happens if the learning rate is increased by several orders of magnitude? How does it affect the loss? Can you explain why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:** \n",
    "\n",
    "**Answer 5:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Visualize class boundaries\n",
    "\n",
    "For this subtask, you will plot the boundary between points (`eruptions`, `waiting` feature pairs) that get classified as severe (+1) or not (-1). To get there, fill in the blanks and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print your learned wts and bias here after training net on standardized samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 6.** What is the meaning of each of the above learned weights with respect to the variables/features in the dataset?\n",
    "\n",
    "*Hint:* Look at your `net_in` equation, look at the features that you feed into the model, look at the scatterplot you made in 1b, think about what features are present in a single training sample.\n",
    "\n",
    "**Question 7.** Which feature / weight index corresponds to the \"y axis value\" in your scatterplot from 1b?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6:** \n",
    "\n",
    "**Answer 7:** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform weights for plotting\n",
    "\n",
    "The class boundary equation is $0 = w_0 + w_1 \\times x_i + w_2 \\times y_i$ for sample $i$ in our data ($i$ goes to 272). But to plot it, we need an equation that looks like $y_i = m \\times x_i + b$ where $m$ and $b$ are some combinations of our weights.\n",
    "\n",
    "1. Scale the weights so that the one corresponding to the \"y value\" is set to 1, then solve for $y$ (*It might be helpful to work this out by hand*). Once you do, adjust the sign/scale of your weights in code so they match up with the equation you wrote out by hand ( of form $y_i = m \\times x_i + b$). **Be careful about the order in which you manipulate the bias and weight values when setting up your equation.**\n",
    "2. Once you're done, have the cell below print your transformed weights/bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = net.get_wts()\n",
    "b = net.get_bias()\n",
    "\n",
    "# TODO: Adjust weights for plotting, then print them here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, sample 50 equally spaced x values from -1.5 to 1.5 for plotting the class boundary. Given the `x_i` values, generate `y_i` values using the equation $y_i = m \\times x_i + b$ (using your transformed weights from above). \n",
    "\n",
    "**Executing the code below should produce a graph that clearly shows this class boundary superimposed on your data scatter plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Perceptron: A neural network with a different activation function\n",
    "\n",
    "In this task, you will apply ADALINE to a larger dataset ([Ionosphere dataset](https://archive.ics.uci.edu/dataset/52/ionosphere)) and compare the performance of ADALINE with another binary classification neural network . A **Perceptron** is a single-layer neural network that works exactly the same as ADALINE, except it uses a different network activation function (`netAct`). The activation function computes the `netAct` as follows:\n",
    "\n",
    "$$\\text{netAct}_i = f(\\text{netIn}_i) = 1  \\text{ if netIn}_i \\geq 0$$\n",
    "$$\\text{netAct}_i = f(\\text{netIn}_i) = -1 \\text{ if netIn}_i < 0$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement and test a Perceptron\n",
    "\n",
    "1. Create a new class in `adaline.py` that will represent your Perceptron classifier. It should inherit from `Adaline`. Override/write any necessary functions. **Hint:** This should be really quick, short, and simple.\n",
    "2. In the cell below, train your Perceptron on the same standardized Old Faithful data by making loss and accuracy plots like you have been using in previous tasks (*though you should replace the default title in `plot_adaline_train` with Perceptron*).\n",
    "\n",
    "If everything is working, you should get very similar results with your Perceptron as above with ADALINE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaline import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Compare performance between ADALINE and Perceptron on Ionosphere dataset\n",
    "\n",
    "Your goal is to train, compare, and analyze the performance of your ADALINE and Perceptron networks on the Ionosphere dataset.\n",
    "\n",
    "The [Ionosphere dataset](https://archive.ics.uci.edu/dataset/52/ionosphere) is radar signal data collected in Goose Bay, Labrador. It is a more complex dataset than old faithful, with 33 features (but still 2 classes). The class values are coded 'g' for good radar signal and 'b' for bad radar signal.\n",
    "\n",
    "**Please download the CSV file from the CS343 project website (not above UCI link)** — I have slightly modified the dataset for your convenience.\n",
    "\n",
    "**TODO:**\n",
    "1. Load in and normalize the Ionosphere dataset. Note that there are no headers in the CSV file and the class values are specified in the last column — **make sure that they are coded properly** (i.e. $-1$ and $+1$).\n",
    "2. In the cell below, use the provided `plot_nets_train` helper function to create a 1x2 plot showing training loss and accuracy of the two networks.\n",
    "\n",
    "I encourage you to play with the hyperparameters, but a good starting point is a learning rate of `1e-5` and 100 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nets_train(loss_lists, acc_lists, net_names, plotMarkers=False):\n",
    "    '''Creates a 1x2 grid of plots showing loss over epochs (left column) and\n",
    "    accuracy over epochs (right column) for one or more network (num_nets in total).\n",
    "    Generalizes `plot_adaline_train` for multiple trained networks.\n",
    "    \n",
    "    For example, in the case of two networks (e.g. adaline and perceptron; num_nets=2),\n",
    "    there would be two curves in each of the two plots.\n",
    "    \n",
    "    Put differently, the following function call would produce the same pair of plots you've\n",
    "    been getting up until this point with a single adaline network:\n",
    "        plot_adaline_train(loss_lists[0], acc_lists[0])\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    loss_lists: Python lists of ndarrays. len(loss_lists) = num_nets. len(loss_lists[0]) = n_epochs.\n",
    "        This would be a list of the loss histories for each of the nets being plotted.\n",
    "    acc_lists: Python lists of ndarrays. len(acc_lists) = num_nets. len(acc_lists[0]) = n_epochs.\n",
    "        This would be a list of the accuracy histories for each of the nets being plotted.\n",
    "    net_names: Python list of str. len(net_names) = num_nets.\n",
    "        Identifying names of each net (e.g. for legend).\n",
    "    plotMarkers: boolean.\n",
    "        Should we draw a plot marker at each epoch on each curve?\n",
    "    '''\n",
    "    n_nets = len(net_names)\n",
    "    n_epochs = len(loss_lists[0])\n",
    "    \n",
    "    colors = ['orange', 'blue', 'red']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n",
    "    fig.suptitle(f'{n_nets} networks trained for ({n_epochs} epochs)')\n",
    "    \n",
    "    for loss_list, acc_list, color in zip(loss_lists, acc_lists, colors):\n",
    "        x = np.arange(1, n_epochs+1)\n",
    "\n",
    "        curveStr = '-'\n",
    "        if plotMarkers:\n",
    "            curveStr += 'o'\n",
    "\n",
    "        ax1.plot(x, loss_list, curveStr, c=color)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss (Sum squared error)')\n",
    "        ax2.plot(x, acc_list, curveStr, c=color)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "    \n",
    "    plt.legend(net_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "**Question 8.** What accuracy are you able to achieve on the Ionosphere dataset with each networks at the end of training?\n",
    "\n",
    "**Question 9.**<br/>(a) What happens when you increase the learning rate by 1+ order of magnitude in each network?<br/>(b) What happens when you decrease the learning rate by 1+ order of magnitude in each network? *You will likely need to increase the number of training epochs too.*<br/>(c) Interpret the difference in behavior between the two nets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers\n",
    "\n",
    "**Answer 8:** \n",
    "\n",
    "**Answer 9:**  (a)\n",
    "\n",
    "(b)\n",
    "\n",
    "(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
